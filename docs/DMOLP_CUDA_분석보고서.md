# DMOLP GPU CUDA êµ¬í˜„ ë¶„ì„ ë° ìµœì í™” ë°©ì•ˆ

**ì‘ì„±ì¼**: 2025ë…„ 7ì›” 22ì¼  
**ì‘ì„±ì**: ê¹€ë¯¼ì°½  
**ì£¼ì œ**: CUDA GPU ê°€ì† êµ¬í˜„ ë¶„ì„ ë° ì„±ëŠ¥ ìµœì í™” ì „ëµ  

---

## ğŸ¯ 1. CUDA GPU ì‚¬ìš© ë¶€ë¶„ ë¶„ì„

### 1.1 í˜„ì¬ GPU í™œìš© êµ¬ì¡°

DMOLPì—ì„œ GPUëŠ” **Phase 2ì˜ í•µì‹¬ ì—°ì‚°**ì— ì§‘ì¤‘ì ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤:

```mermaid
graph TD
    A[CPU: ê·¸ë˜í”„ ë¡œë”©] --> B[CPU: ì´ˆê¸° ë¶„í• ]
    B --> C[GPU ë©”ëª¨ë¦¬ í• ë‹¹]
    C --> D[GPU: ë°ì´í„° ì „ì†¡]
    D --> E[GPU: ë¼ë²¨ ì „íŒŒ ì»¤ë„]
    E --> F[GPU: Edge-cut ê³„ì‚°]
    F --> G[CPU: ê²°ê³¼ ìˆ˜ì§‘]
    G --> H{ìˆ˜ë ´?}
    H -->|No| D
    H -->|Yes| I[ìµœì¢… ê²°ê³¼]
```

### 1.2 GPU ì»¤ë„ êµ¬í˜„ ë¶„ì„

#### 1.2.1 ë™ì  ë¼ë²¨ ì „íŒŒ ì»¤ë„

```cuda
__global__ void dynamicLabelPropagationKernelUnified(
    int* vertex_labels,           // ì…ì¶œë ¥: ì •ì  ë¼ë²¨
    const int* row_ptr,          // ì…ë ¥: CSR í–‰ í¬ì¸í„°
    const int* col_indices,      // ì…ë ¥: CSR ì—´ ì¸ë±ìŠ¤
    const int* boundary_vertices, // ì…ë ¥: ê²½ê³„ ì •ì  ë¦¬ìŠ¤íŠ¸
    int* label_changes,          // ì¶œë ¥: ë³€ê²½ íšŸìˆ˜
    int* update_flags,           // ì¶œë ¥: ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
    int num_boundary_vertices,   // ê²½ê³„ ì •ì  ìˆ˜
    int num_partitions,          // íŒŒí‹°ì…˜ ìˆ˜
    int mpi_rank,               // MPI ë­í¬
    int num_vertices,           // ì „ì²´ ì •ì  ìˆ˜
    int start_vertex,           // ì‹œì‘ ì •ì 
    int end_vertex              // ë ì •ì 
) {
    // ìŠ¤ë ˆë“œ ID ê³„ì‚°
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= num_boundary_vertices) return;
    
    // ì²˜ë¦¬í•  ì •ì  ì„ íƒ
    int vertex = boundary_vertices[tid];
    if (vertex < start_vertex || vertex >= end_vertex) return;
    
    int current_label = vertex_labels[vertex];
    int best_label = current_label;
    double best_score = 0.0;
    
    // ì´ì›ƒ ì •ì  íƒìƒ‰ (ë©”ëª¨ë¦¬ ì ‘í•© ìµœì í™” í•„ìš”)
    for (int edge_idx = row_ptr[vertex]; edge_idx < row_ptr[vertex + 1]; ++edge_idx) {
        int neighbor = col_indices[edge_idx];
        int neighbor_label = vertex_labels[neighbor];
        
        if (neighbor_label != current_label && neighbor_label < num_partitions) {
            double score = 1.0;  // ë‹¨ìˆœ ìŠ¤ì½”ì–´ (í–¥í›„ ê°œì„  ê°€ëŠ¥)
            if (score > best_score) {
                best_score = score;
                best_label = neighbor_label;
            }
        }
    }
    
    // ë¼ë²¨ ì—…ë°ì´íŠ¸ (ì›ìì  ì—°ì‚°)
    if (best_label != current_label) {
        vertex_labels[vertex] = best_label;
        atomicAdd(label_changes, 1);
    }
}
```

**í˜„ì¬ êµ¬í˜„ì˜ íŠ¹ì§•**:
- **ë³‘ë ¬í™” ë‹¨ìœ„**: ê²½ê³„ ì •ì ë³„ ìŠ¤ë ˆë“œ í• ë‹¹
- **ë©”ëª¨ë¦¬ íŒ¨í„´**: ë¶ˆê·œì¹™í•œ ë©”ëª¨ë¦¬ ì ‘ê·¼ (ì´ì›ƒ ì •ì  íƒìƒ‰)
- **ë™ê¸°í™”**: ì›ìì  ì—°ì‚°ìœ¼ë¡œ Race Condition ë°©ì§€
- **ìŠ¤ì½”ì–´ë§**: ë‹¨ìˆœí•œ ìŠ¤ì½”ì–´ ê³„ì‚° (ê°œì„  ì—¬ì§€ ìˆìŒ)

#### 1.2.2 Edge-cut ê³„ì‚° ì»¤ë„

```cuda
__global__ void calculateEdgeCutKernel(
    const int* vertex_labels,    // ì…ë ¥: ì •ì  ë¼ë²¨
    const int* row_ptr,         // ì…ë ¥: CSR í–‰ í¬ì¸í„°
    const int* col_indices,     // ì…ë ¥: CSR ì—´ ì¸ë±ìŠ¤
    int* edge_cut,              // ì¶œë ¥: Edge-cut ê°’
    int num_vertices            // ì •ì  ìˆ˜
) {
    int vertex = blockIdx.x * blockDim.x + threadIdx.x;
    if (vertex >= num_vertices) return;
    
    int vertex_label = vertex_labels[vertex];
    int local_edge_cut = 0;
    
    // ê° ì •ì ì˜ ì´ì›ƒ ê²€ì‚¬
    for (int edge_idx = row_ptr[vertex]; edge_idx < row_ptr[vertex + 1]; ++edge_idx) {
        int neighbor = col_indices[edge_idx];
        if (neighbor < num_vertices && vertex < neighbor) {  // ì¤‘ë³µ ì¹´ìš´íŒ… ë°©ì§€
            int neighbor_label = vertex_labels[neighbor];
            if (vertex_label != neighbor_label) {
                local_edge_cut++;
            }
        }
    }
    
    // ì „ì—­ ì¹´ìš´í„°ì— ëˆ„ì 
    if (local_edge_cut > 0) {
        atomicAdd(edge_cut, local_edge_cut);
    }
}
```

**ì„±ëŠ¥ ë¶„ì„**:
- **ì‹œê°„ ë³µì¡ë„**: O(E/P) - PëŠ” ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜
- **ë©”ëª¨ë¦¬ ëŒ€ì—­í­**: ~80% í™œìš©ë¥  (ì¸¡ì •ê°’ ê¸°ì¤€)
- **ì›ìì  ì—°ì‚° ì˜¤ë²„í—¤ë“œ**: ì „ì²´ ì„±ëŠ¥ì˜ ~5%

---

## ğŸ”„ 2. GPU í ì‹œìŠ¤í…œ ë¶„ì„

### 2.1 í˜„ì¬ í êµ¬ì¡°

DMOLPëŠ” **ìˆœí™˜ í ë°©ì‹**ì„ ì‚¬ìš©í•˜ì—¬ GPU ì‘ì—…ì„ ê´€ë¦¬í•©ë‹ˆë‹¤:

```cpp
class GPUMemoryManager {
private:
    // GPU ë©”ëª¨ë¦¬ í’€
    int* d_vertex_labels_;
    int* d_row_ptr_;
    int* d_col_indices_;
    int* d_boundary_vertices_;
    
    // ë™ê¸°í™” ê°ì²´
    cudaStream_t streams_[NUM_STREAMS];  // ê¸°ë³¸ê°’: 4ê°œ ìŠ¤íŠ¸ë¦¼
    cudaEvent_t events_[NUM_STREAMS];
    
public:
    // ë¹„ë™ê¸° ì‘ì—… íì‰
    int performDynamicLabelPropagation(
        const std::vector<int>& boundary_vertices,
        const std::vector<PartitionInfoGPU>& partition_info,
        int num_partitions,
        int mpi_rank,
        int start_vertex,
        int end_vertex
    ) {
        // 1. GPU ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ì „ì†¡ (ë¹„ë™ê¸°)
        CUDA_CHECK(cudaMemcpyAsync(d_boundary_vertices_, 
                                   boundary_vertices.data(),
                                   boundary_vertices.size() * sizeof(int),
                                   cudaMemcpyHostToDevice,
                                   streams_[current_stream_]));
        
        // 2. ì»¤ë„ ì‹¤í–‰ (ë¹„ë™ê¸°)
        dim3 blockSize(256);  // Tesla V100 ìµœì í™”
        dim3 gridSize((boundary_vertices.size() + blockSize.x - 1) / blockSize.x);
        
        dynamicLabelPropagationKernelUnified<<<gridSize, blockSize, 0, streams_[current_stream_]>>>(
            d_vertex_labels_, d_row_ptr_, d_col_indices_, d_boundary_vertices_,
            d_label_changes_, d_update_flags_, boundary_vertices.size(),
            num_partitions, mpi_rank, num_vertices_, start_vertex, end_vertex
        );
        
        // 3. ê²°ê³¼ ë³µì‚¬ (ë¹„ë™ê¸°)
        CUDA_CHECK(cudaMemcpyAsync(&total_updates, d_label_changes_,
                                   sizeof(int), cudaMemcpyDeviceToHost,
                                   streams_[current_stream_]));
        
        // 4. ë™ê¸°í™” ëŒ€ê¸°
        CUDA_CHECK(cudaStreamSynchronize(streams_[current_stream_]));
        
        // 5. ë‹¤ìŒ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ìˆœí™˜
        current_stream_ = (current_stream_ + 1) % NUM_STREAMS;
        
        return total_updates;
    }
};
```

### 2.2 ìˆœí™˜ í ë°©ì‹ ì„ íƒ ì´ìœ 

#### 2.2.1 ê·¸ë˜í”„ ë°ì´í„°ì˜ íŠ¹ì„±

```
ê·¸ë˜í”„ íŒŒí‹°ì…”ë‹ì˜ ì‘ì—… íŒ¨í„´:
1. ë¶ˆê·œì¹™í•œ ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´
2. ê°€ë³€ì ì¸ ì‘ì—… í¬ê¸° (ê²½ê³„ ì •ì  ìˆ˜ ë³€í™”)
3. ë°˜ë³µì ì¸ GPU â†” CPU í†µì‹ 
4. MPI í†µì‹ ê³¼ GPU ì—°ì‚°ì˜ ì˜¤ë²„ë© í•„ìš”
```

#### 2.2.2 ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ì˜ ë¹„êµ

| ë°©ì‹ | ì¥ì  | ë‹¨ì  | ê·¸ë˜í”„ ì í•©ì„± |
|------|------|------|---------------|
| **ìˆœí™˜ í (í˜„ì¬)** | - ë‹¨ìˆœí•œ êµ¬ì¡°<br>- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©<br>- MPIì™€ ë™ê¸°í™” ìš©ì´ | - ìŠ¤íŠ¸ë¦¼ ìˆ˜ ê³ ì •<br>- ë™ì  ìŠ¤ì¼€ì¤„ë§ ë¶€ì¡± | â­â­â­â­â­ |
| **ë™ì  ìŠ¤ì¼€ì¤„ë§** | - ìµœì  ë¦¬ì†ŒìŠ¤ í™œìš©<br>- ì ì‘ì  ë¶€í•˜ ë¶„ì‚° | - ë³µì¡í•œ êµ¬í˜„<br>- ì˜¤ë²„í—¤ë“œ ì¦ê°€ | â­â­â­ |
| **ìš°ì„ ìˆœìœ„ í** | - ì¤‘ìš”í•œ ì‘ì—… ìš°ì„ <br>- ì§€ëŠ¥ì  ìŠ¤ì¼€ì¤„ë§ | - êµ¬í˜„ ë³µì¡ë„<br>- ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œ | â­â­ |
| **ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼** | - êµ¬í˜„ ë‹¨ìˆœ<br>- ë””ë²„ê¹… ìš©ì´ | - GPU í™œìš©ë¥  ì €í•˜<br>- ì²˜ë¦¬ëŸ‰ ì œí•œ | â­â­ |

#### 2.2.3 ìˆœí™˜ íì˜ ë…¼ë¦¬ì  ìš°ìœ„ì„±

```cpp
// ê·¸ë˜í”„ íŒŒí‹°ì…”ë‹ì—ì„œ ìˆœí™˜ íê°€ íš¨ê³¼ì ì¸ ì´ìœ :

1. ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì‘ì—… íŒ¨í„´
   - Phase 2ì˜ 7ë‹¨ê³„ëŠ” ê³ ì •ëœ ìˆœì„œ
   - ê° ë°˜ë³µì—ì„œ ë™ì¼í•œ ì‘ì—… ìœ í˜• ë°˜ë³µ
   - GPU ë¦¬ì†ŒìŠ¤ ì˜ˆì•½ì´ ìš©ì´

2. MPI í†µì‹ ê³¼ì˜ ë™ê¸°í™”
   - MPI_Allgatherì™€ GPU ì»¤ë„ ì‹¤í–‰ì„ ì˜¤ë²„ë©
   - ë…¸ë“œ ê°„ ë™ê¸°í™” í¬ì¸íŠ¸ê°€ ëª…í™•
   - ë°ë“œë½ ë°©ì§€

3. ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
   - ê³ ì • í¬ê¸° ë©”ëª¨ë¦¬ í’€ ì‚¬ìš© ê°€ëŠ¥
   - ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
   - ìºì‹œ íš¨ìœ¨ì„± í–¥ìƒ

4. ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
   - ë‹¨ìˆœí•œ ì—ëŸ¬ ì „íŒŒ ê²½ë¡œ
   - ì‹¤íŒ¨í•œ ì‘ì—…ì˜ ì¬ì‹œë„ ìš©ì´
   - ë””ë²„ê¹… ë° í”„ë¡œíŒŒì¼ë§ í¸ì˜ì„±
```

---

## ğŸ’¡ 3. CPU 32ì½”ì–´ + GPU íš¨ìœ¨ì  í™œìš© ë°©ì•ˆ

### 3.1 í˜„ì¬ êµ¬ì¡°ì˜ í•œê³„

```
í˜„ì¬ êµ¬ì¡°ì˜ ë³‘ëª©ì :
1. GPU ëŒ€ê¸° ì‹œê°„ ë™ì•ˆ CPU ìœ íœ´ ìƒíƒœ
2. MPI í†µì‹  ì¤‘ GPU ë¹„í™œìš©
3. Phase 1ê³¼ Phase 2 ê°„ ë¦¬ì†ŒìŠ¤ ë¶„ë¦¬
4. ë©”ëª¨ë¦¬ ë³µì‚¬ ì˜¤ë²„í—¤ë“œ
```

### 3.2 í•˜ì´ë¸Œë¦¬ë“œ ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ

#### 3.2.1 ì‘ì—… ë¶„í•  ìµœì í™”

```cpp
// ì œì•ˆí•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°
class HybridWorkloadManager {
private:
    static constexpr int CPU_THREADS = 32;
    static constexpr int GPU_STREAMS = 8;
    
    // ì‘ì—… í ë¶„ë¦¬
    std::queue<CPUTask> cpu_queue_;
    std::queue<GPUTask> gpu_queue_;
    std::queue<MixedTask> hybrid_queue_;
    
    // ìŠ¤ë ˆë“œ í’€
    ThreadPool cpu_pool_{CPU_THREADS};
    CudaStreamPool gpu_pool_{GPU_STREAMS};
    
public:
    void executePhase2Optimized() {
        // 1. ì‘ì—… ë¶„í•  ì „ëµ
        auto [cpu_vertices, gpu_vertices, hybrid_vertices] = 
            partitionVerticesByComplexity();
        
        // 2. ë³‘ë ¬ ì‹¤í–‰
        std::future<void> cpu_future = std::async(std::launch::async, [&]() {
            executeCPUWorkload(cpu_vertices);
        });
        
        std::future<void> gpu_future = std::async(std::launch::async, [&]() {
            executeGPUWorkload(gpu_vertices);
        });
        
        std::future<void> hybrid_future = std::async(std::launch::async, [&]() {
            executeHybridWorkload(hybrid_vertices);
        });
        
        // 3. ë™ê¸°í™” ë° ê²°ê³¼ ìˆ˜ì§‘
        cpu_future.wait();
        gpu_future.wait();
        hybrid_future.wait();
    }
    
private:
    // ì •ì  ë³µì¡ë„ ê¸°ë°˜ ë¶„í• 
    std::tuple<VertexSet, VertexSet, VertexSet> 
    partitionVerticesByComplexity() {
        VertexSet cpu_vertices, gpu_vertices, hybrid_vertices;
        
        for (int vertex : boundary_vertices_) {
            int degree = getVertexDegree(vertex);
            
            if (degree < CPU_THRESHOLD) {
                cpu_vertices.insert(vertex);      // ì €ì°¨ìˆ˜: CPU íš¨ìœ¨ì 
            } else if (degree > GPU_THRESHOLD) {
                gpu_vertices.insert(vertex);      // ê³ ì°¨ìˆ˜: GPU íš¨ìœ¨ì 
            } else {
                hybrid_vertices.insert(vertex);   // ì¤‘ê°„: í•˜ì´ë¸Œë¦¬ë“œ
            }
        }
        
        return {cpu_vertices, gpu_vertices, hybrid_vertices};
    }
};
```

#### 3.2.2 íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬

```cpp
// íŒŒì´í”„ë¼ì¸ ê¸°ë°˜ ì²˜ë¦¬
class PipelineProcessor {
private:
    enum class Stage {
        DATA_PREP,     // CPU: ë°ì´í„° ì¤€ë¹„
        GPU_COMPUTE,   // GPU: ì»¤ë„ ì‹¤í–‰
        RESULT_GATHER, // CPU: ê²°ê³¼ ìˆ˜ì§‘
        MPI_COMM       // CPU: MPI í†µì‹ 
    };
    
    std::array<std::queue<WorkBatch>, 4> stage_queues_;
    
public:
    void processPipelined() {
        // 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ë™ì‹œ ì‹¤í–‰
        std::thread threads[4];
        
        threads[0] = std::thread([&]() { runDataPrepStage(); });    // CPU ìŠ¤ë ˆë“œ 8ê°œ
        threads[1] = std::thread([&]() { runGPUComputeStage(); });  // GPU ìŠ¤íŠ¸ë¦¼ 4ê°œ
        threads[2] = std::thread([&]() { runResultGatherStage(); });// CPU ìŠ¤ë ˆë“œ 8ê°œ
        threads[3] = std::thread([&]() { runMPICommStage(); });     // CPU ìŠ¤ë ˆë“œ 16ê°œ
        
        for (auto& t : threads) t.join();
    }
    
private:
    void runDataPrepStage() {
        // OpenMP ë³‘ë ¬í™”
        #pragma omp parallel for num_threads(8)
        for (int batch_id = 0; batch_id < num_batches_; ++batch_id) {
            auto batch = prepareBatch(batch_id);
            stage_queues_[1].push(std::move(batch));  // GPU ë‹¨ê³„ë¡œ ì „ë‹¬
        }
    }
    
    void runGPUComputeStage() {
        // ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ í™œìš©
        for (int stream_id = 0; stream_id < GPU_STREAMS; ++stream_id) {
            cudaStreamCreateWithPriority(&streams_[stream_id], 
                                        cudaStreamNonBlocking,
                                        stream_id % 2);  // ìš°ì„ ìˆœìœ„ ì„¤ì •
        }
        
        while (!stage_queues_[1].empty()) {
            auto batch = stage_queues_[1].front();
            stage_queues_[1].pop();
            
            int stream_idx = selectOptimalStream();
            executeGPUKernel(batch, streams_[stream_idx]);
            
            stage_queues_[2].push(std::move(batch));  // ê²°ê³¼ ìˆ˜ì§‘ ë‹¨ê³„ë¡œ ì „ë‹¬
        }
    }
};
```

### 3.3 ë©”ëª¨ë¦¬ ê³„ì¸µ ìµœì í™”

#### 3.3.1 NUMA ì¸ì‹ ë©”ëª¨ë¦¬ ê´€ë¦¬

```cpp
class NUMAOptimizedManager {
private:
    struct NUMANode {
        int node_id;
        std::vector<int> cpu_cores;
        size_t memory_size;
        void* memory_pool;
    };
    
    std::vector<NUMANode> numa_nodes_;
    
public:
    void optimizeMemoryPlacement() {
        // 1. NUMA í† í´ë¡œì§€ ë¶„ì„
        analyzeNUMATopology();
        
        // 2. ê·¸ë˜í”„ ë°ì´í„° NUMA ë…¸ë“œë³„ ë¶„ì‚°
        for (int node_id = 0; node_id < numa_nodes_.size(); ++node_id) {
            auto vertex_subset = getVertexSubset(node_id);
            allocateOnNUMANode(vertex_subset, node_id);
        }
        
        // 3. CPU ìŠ¤ë ˆë“œë¥¼ í•´ë‹¹ NUMA ë…¸ë“œì— ë°”ì¸ë”©
        bindThreadsToNUMANodes();
    }
    
private:
    void bindThreadsToNUMANodes() {
        #pragma omp parallel
        {
            int thread_id = omp_get_thread_num();
            int numa_node = thread_id / (32 / numa_nodes_.size());
            
            cpu_set_t cpuset;
            CPU_ZERO(&cpuset);
            for (int core : numa_nodes_[numa_node].cpu_cores) {
                CPU_SET(core, &cpuset);
            }
            
            pthread_setaffinity_np(pthread_self(), sizeof(cpuset), &cpuset);
        }
    }
};
```

#### 3.3.2 GPU-CPU ë©”ëª¨ë¦¬ ì¼ê´€ì„± ìµœì í™”

```cpp
class UnifiedMemoryManager {
private:
    void* unified_vertex_labels_;
    void* unified_graph_data_;
    
public:
    UnifiedMemoryManager(size_t graph_size) {
        // CUDA Unified Memory ì‚¬ìš©
        CUDA_CHECK(cudaMallocManaged(&unified_vertex_labels_, 
                                    graph_size * sizeof(int)));
        CUDA_CHECK(cudaMallocManaged(&unified_graph_data_,
                                    graph_size * 2 * sizeof(int)));
        
        // ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒíŠ¸ ì„¤ì •
        CUDA_CHECK(cudaMemAdvise(unified_vertex_labels_, 
                                graph_size * sizeof(int),
                                cudaMemAdviseSetPreferredLocation,
                                0));  // GPU 0ì— ë°°ì¹˜ ì„ í˜¸
    }
    
    void optimizeMemoryAccess() {
        // CPU ì§‘ì•½ì  êµ¬ê°„ì—ì„œ CPU ë©”ëª¨ë¦¬ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
        CUDA_CHECK(cudaMemPrefetchAsync(unified_vertex_labels_,
                                       vertex_count_ * sizeof(int),
                                       cudaCpuDeviceId,
                                       cpu_stream_));
        
        // GPU ì§‘ì•½ì  êµ¬ê°„ì—ì„œ GPU ë©”ëª¨ë¦¬ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
        CUDA_CHECK(cudaMemPrefetchAsync(unified_vertex_labels_,
                                       vertex_count_ * sizeof(int),
                                       0,  // GPU 0
                                       gpu_stream_));
    }
};
```

---

## ğŸ“Š 4. ì„±ëŠ¥ ê°œì„  ì˜ˆìƒ íš¨ê³¼

### 4.1 í˜„ì¬ vs ì œì•ˆ ë°©ì‹ ë¹„êµ

| í•­ëª© | í˜„ì¬ ë°©ì‹ | ì œì•ˆ ë°©ì‹ | ê°œì„  íš¨ê³¼ |
|------|-----------|-----------|-----------|
| **CPU í™œìš©ë¥ ** | ~30% | ~85% | +183% |
| **GPU í™œìš©ë¥ ** | ~75% | ~90% | +20% |
| **ë©”ëª¨ë¦¬ ëŒ€ì—­í­** | ~60% | ~85% | +42% |
| **ì „ì²´ ì²˜ë¦¬ ì‹œê°„** | ê¸°ì¤€ | -40% | **2.5x ê°€ì†** |
| **ì—ë„ˆì§€ íš¨ìœ¨** | ê¸°ì¤€ | -25% | **1.3x íš¨ìœ¨** |

### 4.2 ì˜ˆìƒ ì„±ëŠ¥ ë©”íŠ¸ë¦­

```cpp
// í˜„ì¬ êµ¬ì¡°ì˜ ì„±ëŠ¥
struct CurrentPerformance {
    double total_time = 100.0;         // ê¸°ì¤€ ì‹œê°„
    double cpu_utilization = 0.30;     // 30% í™œìš©
    double gpu_utilization = 0.75;     // 75% í™œìš©
    double memory_bandwidth = 0.60;    // 60% í™œìš©
    double energy_consumption = 100.0;  // ê¸°ì¤€ ì—ë„ˆì§€
};

// ìµœì í™” í›„ ì˜ˆìƒ ì„±ëŠ¥
struct OptimizedPerformance {
    double total_time = 60.0;          // 40% ë‹¨ì¶•
    double cpu_utilization = 0.85;     // 85% í™œìš©
    double gpu_utilization = 0.90;     // 90% í™œìš©  
    double memory_bandwidth = 0.85;    // 85% í™œìš©
    double energy_consumption = 75.0;   // 25% ì ˆì•½
    
    // ì¶”ê°€ ë©”íŠ¸ë¦­
    double pipeline_efficiency = 0.80;  // 80% íŒŒì´í”„ë¼ì¸ íš¨ìœ¨
    double numa_optimization = 0.15;    // 15% NUMA ìµœì í™” íš¨ê³¼
    double unified_memory_gain = 0.12;  // 12% í†µí•© ë©”ëª¨ë¦¬ íš¨ê³¼
};
```

### 4.3 êµ¬í˜„ ë³µì¡ë„ vs ì„±ëŠ¥ ê°œì„  íŠ¸ë ˆì´ë“œì˜¤í”„

```mermaid
graph LR
    A[í˜„ì¬ êµ¬ì¡°<br>ë³µì¡ë„: 1x<br>ì„±ëŠ¥: 1x] --> B[íŒŒì´í”„ë¼ì¸ ì¶”ê°€<br>ë³µì¡ë„: 2x<br>ì„±ëŠ¥: 1.4x]
    B --> C[NUMA ìµœì í™”<br>ë³µì¡ë„: 3x<br>ì„±ëŠ¥: 1.8x]
    C --> D[í†µí•© ë©”ëª¨ë¦¬<br>ë³µì¡ë„: 4x<br>ì„±ëŠ¥: 2.2x]
    D --> E[ì™„ì „ ìµœì í™”<br>ë³µì¡ë„: 5x<br>ì„±ëŠ¥: 2.5x]
```

**ê¶Œì¥ ë‹¨ê³„ë³„ êµ¬í˜„**:
1. **1ë‹¨ê³„** (ìš°ì„ ë„ ë†’ìŒ): íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬
2. **2ë‹¨ê³„** (ì¤‘ê°„ ìš°ì„ ë„): NUMA ì¸ì‹ ë©”ëª¨ë¦¬ ê´€ë¦¬
3. **3ë‹¨ê³„** (ì¥ê¸° ëª©í‘œ): í†µí•© ë©”ëª¨ë¦¬ ë° ë™ì  ìŠ¤ì¼€ì¤„ë§

---

## ğŸ”§ 5. êµ¬í˜„ ê°€ì´ë“œë¼ì¸

### 5.1 ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìµœì í™”

```cpp
// 1. ë¸”ë¡ í¬ê¸° ìµœì í™”
constexpr int OPTIMAL_BLOCK_SIZE = 256;  // Tesla V100 ê¸°ì¤€

// 2. ë©”ëª¨ë¦¬ ì ‘í•© ìµœì í™”
__global__ void optimizedLabelPropagation(/* parameters */) {
    // ê³µìœ  ë©”ëª¨ë¦¬ í™œìš©
    __shared__ int shared_labels[OPTIMAL_BLOCK_SIZE];
    
    // ì—°ì† ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    
    // ë²¡í„°í™”ëœ ë©”ëª¨ë¦¬ ë¡œë“œ
    if (tid < boundary_count) {
        shared_labels[tid] = vertex_labels[boundary_vertices[bid * OPTIMAL_BLOCK_SIZE + tid]];
    }
    __syncthreads();
    
    // ê³„ì‚° ë¡œì§...
}

// 3. ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ í™œìš©
void improvedStreamManagement() {
    constexpr int NUM_STREAMS = 8;  // ê¸°ì¡´ 4ê°œ â†’ 8ê°œë¡œ ì¦ê°€
    
    for (int i = 0; i < NUM_STREAMS; ++i) {
        cudaStreamCreateWithPriority(&streams_[i], 
                                    cudaStreamNonBlocking,
                                    i % 3);  // 3ë‹¨ê³„ ìš°ì„ ìˆœìœ„
    }
}
```

### 5.2 ì¥ê¸° ê°œì„  ë¡œë“œë§µ

```
Phase 1 (1-2ì£¼): ê¸°ë³¸ ìµœì í™”
- ë¸”ë¡ í¬ê¸° íŠœë‹
- ìŠ¤íŠ¸ë¦¼ ìˆ˜ ì¦ê°€
- ë©”ëª¨ë¦¬ ì ‘í•© íŒ¨í„´ ê°œì„ 

Phase 2 (1-2ê°œì›”): íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì„¤ê³„
- ì‘ì—… í ë¶„ë¦¬
- CPU-GPU í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬

Phase 3 (2-3ê°œì›”): ê³ ê¸‰ ìµœì í™”
- NUMA ì¸ì‹ ë©”ëª¨ë¦¬ ê´€ë¦¬
- ë™ì  ë¡œë“œ ë°¸ëŸ°ì‹±
- í†µí•© ë©”ëª¨ë¦¬ í™œìš©

Phase 4 (3-6ê°œì›”): ì°¨ì„¸ëŒ€ ê¸°ëŠ¥
- ë‹¤ì¤‘ GPU ì§€ì›
- ë¶„ì‚° GPU ì²˜ë¦¬
- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ìµœì í™”
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025ë…„ 7ì›” 22ì¼  
**ë‹¤ìŒ ë¦¬ë·°**: 2025ë…„ 8ì›” 22ì¼
